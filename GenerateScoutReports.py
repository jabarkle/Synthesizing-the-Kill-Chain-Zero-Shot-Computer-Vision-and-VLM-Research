#!/usr/bin/env python3
"""
Generate Scout Reports Markdown
Creates a detailed markdown file showing what each model's Agent 1 reported
for each objective in each scenario.
"""

import json
from pathlib import Path

RESULTS_FOLDER = "Results"
INPUT_FILE = "AgentEval_E2E_results.json"
OUTPUT_FILE = "AgentEval_E2E_scout_reports.md"


def load_results(filepath):
    """Load evaluation results from JSON"""
    with open(filepath, 'r') as f:
        return json.load(f)


def generate_scout_reports_markdown(results, output_path):
    """Generate markdown showing all scout reports by model and scenario"""

    lines = [
        "# Agent Evaluation: Scout Drone Reports",
        "",
        f"Generated: {results['timestamp']}",
        "",
        "This document contains the individual scout reports generated by Agent 1 (the reconnaissance drone)",
        "for each objective in each scenario. These reports were then aggregated and sent to Agent 2",
        "(the command & control node) for tactical decision-making.",
        "",
        "**Ground Truth:**",
        "- OBJECTIVE A: Operational Main Battle Tank (correct target)",
        "- OBJECTIVE B: Truck/Transport vehicle (false positive, not a valid target)",
        "- OBJECTIVE C: Destroyed tank (no threat)",
        "",
        "---",
        ""
    ]

    # Get model names in order
    model_names = list(results["models"].keys())

    for model_name in model_names:
        model_data = results["models"][model_name]
        summary = model_data["summary"]

        lines.append(f"# {model_name}")
        lines.append("")
        lines.append(f"**Overall Performance:** {summary['correct_decisions']}/{summary['total_scenarios']} correct ({summary['accuracy_percent']}%)")
        lines.append(f"**Average Reasoning Score:** {summary.get('avg_reasoning_score', 'N/A')}/10")
        lines.append("")
        lines.append("---")
        lines.append("")

        for scenario in model_data["scenario_results"]:
            if not scenario.get("success", False):
                continue

            scenario_num = scenario["scenario_num"]
            selected = scenario["selected_objective"]
            is_correct = scenario.get("is_correct", False)
            status = "CORRECT" if is_correct else "INCORRECT"

            lines.append(f"## Scenario {scenario_num}")
            lines.append("")
            lines.append(f"**Final Decision:** {selected} ({status})")
            lines.append(f"**Scenario Time:** {scenario['scenario_time_sec']}s")
            lines.append("")

            # Scout reports for each objective
            scout_reports = scenario.get("scout_reports", {})

            for objective in ["OBJECTIVE A", "OBJECTIVE B", "OBJECTIVE C"]:
                report = scout_reports.get(objective, {})

                # Determine ground truth label
                if objective == "OBJECTIVE A":
                    ground_truth = "(Ground Truth: Operational MBT - CORRECT TARGET)"
                elif objective == "OBJECTIVE B":
                    ground_truth = "(Ground Truth: Truck - NOT a valid target)"
                else:
                    ground_truth = "(Ground Truth: Destroyed Tank - NO threat)"

                lines.append(f"### {objective}")
                lines.append(f"*{ground_truth}*")
                lines.append("")
                lines.append(f"**Vehicle Type:** {report.get('vehicle_type', 'N/A')}")
                lines.append("")
                lines.append(f"**Operational Status:** {report.get('operational_status', 'N/A')}")
                lines.append("")
                lines.append(f"**Description:**")
                lines.append(f"> {report.get('description', 'N/A')}")
                lines.append("")

            lines.append("---")
            lines.append("")

    with open(output_path, 'w') as f:
        f.write('\n'.join(lines))

    print(f"Scout reports markdown saved to: {output_path}")


def run():
    script_dir = Path(__file__).parent
    results_dir = script_dir / RESULTS_FOLDER
    input_path = results_dir / INPUT_FILE
    output_path = results_dir / OUTPUT_FILE

    if not input_path.exists():
        print(f"ERROR: Results file not found: {input_path}")
        print("Please run AgentEval.py first to generate results.")
        return

    results = load_results(input_path)
    generate_scout_reports_markdown(results, output_path)

    print("\nDone!")


if __name__ == "__main__":
    run()
